{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjJ9Wjl8jFoG",
        "outputId": "4f11784a-2d6d-47b7-a468-e0d1a1316ecd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quHoV8zgZ-qc"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smLCAoxLZ8pO"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from evaluate import load\n",
        "import tqdm\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEEhQHd2ZDBO"
      },
      "outputs": [],
      "source": [
        "def load_data(golds, preds):\n",
        "    with open(golds) as f:\n",
        "        gold_labels = json.load(f)\n",
        "\n",
        "    with open(preds) as f:\n",
        "        predictions = json.load(f)\n",
        "\n",
        "    return gold_labels, predictions\n",
        "\n",
        "\n",
        "def extract_prediction(pred, task):\n",
        "    if task == 'reviewText':\n",
        "        pattern = r'\\[\\[VIDEOID:[^\\]]*\\]\\]'\n",
        "        noid = re.sub(pattern, '', pred)\n",
        "\n",
        "        if '**Review text:** \"' in noid:\n",
        "            return noid.split('**Review text:** \"')[1].rsplit('\"', 1)[0].strip()\n",
        "        elif 'Review text: \"' in noid:\n",
        "            return noid.split('Review text: \"')[1].rsplit('\"', 1)[0].strip()\n",
        "        elif 'Review text: ' in noid:\n",
        "            return noid.split('Review text: ')[1].strip()\n",
        "        elif 'The review text is: \\\"' in noid:\n",
        "            return noid.split('The review text is: \\\"')[1].rsplit('\\\"', 1)[0].strip()\n",
        "        elif 'The review text is: \"' in noid:\n",
        "            return noid.split('The review text is: \"')[1].rsplit('\"', 1)[0].strip()\n",
        "        elif 'The review text is: ' in noid:\n",
        "            return noid.split('The review text is: ')[1].strip()\n",
        "        else:\n",
        "            return noid.strip()\n",
        "\n",
        "    elif task == 'reviewTitle':\n",
        "        if '**Review title:** \"\"' in pred:\n",
        "            return pred.split('**Review title:** \"')[1].rsplit('\"', 1)[0].strip()\n",
        "        elif 'Review title: ' in pred:\n",
        "            return pred.split('Review title: ')[1].strip()\n",
        "        else:\n",
        "            return pred.strip()\n",
        "\n",
        "    elif task == 'reviewRating':\n",
        "        if '**Review rating:** \"\"' in pred:\n",
        "            return pred.split('**Review rating:** \"')[1].rsplit('\"', 1)[0].strip()\n",
        "        elif 'Review rating: ' in pred:\n",
        "            return pred.split('Review rating: ')[1].strip()\n",
        "        elif 'Rating: ' in pred:\n",
        "            return pred.split('Rating: ')[1].strip()\n",
        "        else:\n",
        "            return pred.strip()\n",
        "\n",
        "\n",
        "# def clean_data(golds, preds, neighbors=None, model=None, dic=None):\n",
        "def clean_data(golds, preds, task):\n",
        "    clean_golds = []\n",
        "    clean_preds = []\n",
        "\n",
        "    if isinstance(preds[0], str): # just future proofing\n",
        "        flag = 'str'\n",
        "    else:\n",
        "        flag = 'obj'\n",
        "\n",
        "    if task == 'reviewText':\n",
        "        field = 'user_review_text'\n",
        "    elif task == 'reviewTitle':\n",
        "        field = 'user_review_title'\n",
        "    elif task == 'reviewRating':\n",
        "        field = 'user_review_rating'\n",
        "\n",
        "    if flag == 'str': # user id not included\n",
        "\n",
        "        for i, pred in enumerate(preds):\n",
        "            # cleaned.append({\"gold\": golds[i][field].strip(), \"pred\": extract_prediction(pred, task)})\n",
        "            clean_golds.append(golds[i][field].strip())\n",
        "            clean_preds.append(extract_prediction(pred, task))\n",
        "\n",
        "    else: # user id included\n",
        "\n",
        "        for i, pred in enumerate(preds):\n",
        "            # cleaned.append({\"id\": pred['user_id'], \"gold\": golds[i][field].strip(), \"pred\": extract_prediction(pred[\"output\"], task)})\n",
        "            clean_golds.append(golds[i][field].strip())\n",
        "            clean_preds.append(extract_prediction(pred[\"output\"], task))\n",
        "\n",
        "    return clean_golds, clean_preds\n",
        "\n",
        "def full_eval(golds, preds, mode_k, rouge, meteor):\n",
        "    # rouge = load('rouge')\n",
        "    # meteor = load('meteor')\n",
        "\n",
        "    rouge_results = rouge.compute(predictions=preds, references=golds)\n",
        "    meteor_results = meteor.compute(predictions=preds, references=golds)\n",
        "\n",
        "    # results = {\n",
        "    #     \"mode_k\": mode_k,\n",
        "    #     \"rouge-1\": round(rouge_results[\"rouge1\"], 3),\n",
        "    #     \"rouge-L\": round(rouge_results[\"rougeL\"], 3),\n",
        "    #     \"meteor\": round(meteor_results[\"meteor\"], 3)\n",
        "    # }\n",
        "\n",
        "    results = [\n",
        "        mode_k,\n",
        "        round(rouge_results[\"rouge1\"], 3),\n",
        "        round(rouge_results[\"rougeL\"], 3),\n",
        "        round(meteor_results[\"meteor\"], 3)\n",
        "        ]\n",
        "\n",
        "    return results\n",
        "\n",
        "def dump_results(results, dataset, split, task, model, ranker, directory):\n",
        "    header_line = f'Evaluation report for {dataset}_{split}_{task}_{model}_{ranker}'\n",
        "    data_headers = ['mode_k', 'ROUGE-1', 'ROUGE-L', 'METEOR']\n",
        "\n",
        "    filename = f'EVAL-{dataset}_{split}_{task}_{model}_{ranker}.csv'\n",
        "    file_path = os.path.join(directory, filename)\n",
        "    with open(file_path, 'w', newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "\n",
        "        writer.writerow([header_line])\n",
        "        writer.writerow([])\n",
        "        writer.writerow(data_headers)\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(f\"Evaluation results saved to {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNMYNywCaC7e"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description=\"Evaluation Pipeline\")\n",
        "    parser.add_argument('--ranking', type=str, required=True, help=\"Path to input data file\")\n",
        "    parser.add_argument('--results', type=str, required=True, help=\"Path to results file\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U73ofmn2aFDy"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    args = parse_arguments()\n",
        "\n",
        "    ranking_file = os.path.splitext(os.path.basename(args.ranking))[0]\n",
        "\n",
        "    # Remove the 'RANKED-' prefix\n",
        "    if ranking_file.startswith(\"RANKING-\"):\n",
        "        ranking_file = ranking_file[len(\"RANKING-\"):]\n",
        "    # parse run information from filename\n",
        "    parsed = ranking_file.split('_')\n",
        "\n",
        "    dataset = parsed[0]\n",
        "    split = parsed[1]\n",
        "    task = parsed[2]\n",
        "    ranker = parsed[3]\n",
        "\n",
        "    model = args.results.split('_')[3] # pulls model used for output\n",
        "\n",
        "    rouge = load('rouge')\n",
        "    meteor = load('meteor')\n",
        "\n",
        "    if os.path.isdir(args.results):\n",
        "        directory = args.results\n",
        "        # Handle the case where input_path is a directory\n",
        "        all_metrics = []\n",
        "        for filename in os.listdir(args.results):\n",
        "            file_path = os.path.join(args.results, filename)\n",
        "            if os.path.isfile(file_path):\n",
        "                currfile = os.path.splitext(os.path.basename(file_path))[0]\n",
        "                mode_k = currfile.split('-')[-1] # pulls the mode and k from filename\n",
        "\n",
        "                golds, preds = load_data(args.ranking, file_path)\n",
        "                clean_golds, clean_preds = clean_data(golds, preds, task)\n",
        "                all_metrics.append(full_eval(clean_golds, clean_preds, mode_k, rouge, meteor))\n",
        "\n",
        "    elif os.path.isfile(args.results):\n",
        "        directory = os.path.dirname(file_path)\n",
        "        # Handle the case where input_path is a single file\n",
        "        filename = os.path.splitext(os.path.basename(args.ranking))[0]\n",
        "        mode_k = filename.split('-')[-1]\n",
        "\n",
        "        golds, preds = load_data(args.ranking, filename)\n",
        "        clean_golds, clean_preds = clean_data(golds, preds, task)\n",
        "        all_metrics = [full_eval(clean_golds, clean_preds, mode_k, rouge, meteor)]\n",
        "\n",
        "    else:\n",
        "        print(f\"Error: The provided path '{args.results}' is neither a file nor a directory.\")\n",
        "\n",
        "\n",
        "    dump_results(all_metrics, dataset, split, task, model, ranker, directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QliTFlsWaH0A"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.argv = ['master_eval.py', '--ranking', '/content/drive/FilePath', '--results', '/content/drive/FileDestination']#, '--neighbors', '2']\n",
        "\n",
        "args = parse_arguments()\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
